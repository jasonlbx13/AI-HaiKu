{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T06:17:29.028909Z",
     "start_time": "2020-03-17T06:17:26.846150Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import keras\n",
    "except:\n",
    "    !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T06:17:31.464179Z",
     "start_time": "2020-03-17T06:17:31.082600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "tf_session = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(tf_session)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,  CSVLogger\n",
    "from keras.layers import Add, Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Local library with model definitions for training and generating\n",
    "from models import Generator, create_training_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T06:17:56.455823Z",
     "start_time": "2020-03-17T06:17:56.450731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Percent of samples to use for training, might be necessary if you're running out of memory\n",
    "sample_size = 1\n",
    "\n",
    "# The latent dimension of the LSTM\n",
    "latent_dim = 2048\n",
    "\n",
    "# Number of epochs to train for\n",
    "epochs = 10\n",
    "\n",
    "root_path = Path('../../..')\n",
    "input_path = root_path / 'input'\n",
    "poem_path = input_path / 'poems'\n",
    "haiku_path = poem_path / 'haikus.csv'\n",
    "\n",
    "name = 'train_file'\n",
    "output_dir = Path('output_%s' % name)\n",
    "output_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:04:49.081534Z",
     "start_time": "2020-03-13T04:04:48.628064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>source</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39745</th>\n",
       "      <td>my uncle HAD to</td>\n",
       "      <td>have a brother who then had</td>\n",
       "      <td>kids what a loser</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82611</th>\n",
       "      <td>Hating on the Browns</td>\n",
       "      <td>isn't gonna get rid of</td>\n",
       "      <td>your man tits Jason</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73965</th>\n",
       "      <td>i'm sorry but if</td>\n",
       "      <td>this happens i'll stop watching</td>\n",
       "      <td>college volleyball</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68681</th>\n",
       "      <td>People gonna treat</td>\n",
       "      <td>you how you allow them to</td>\n",
       "      <td>treat you everytime</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132423</th>\n",
       "      <td>Patrick we should start</td>\n",
       "      <td>working out Me you should start</td>\n",
       "      <td>minding ur business</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50742</th>\n",
       "      <td>Championship game</td>\n",
       "      <td>tomorrow WE ABOUT TO</td>\n",
       "      <td>GET THAT W</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40233</th>\n",
       "      <td>it is okay if</td>\n",
       "      <td>you ignoring me but i</td>\n",
       "      <td>want you notice me</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87743</th>\n",
       "      <td>imagine being</td>\n",
       "      <td>here from the beginning at</td>\n",
       "      <td>witnessing them i</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93534</th>\n",
       "      <td>What a healing and</td>\n",
       "      <td>deliverance praise Indeed</td>\n",
       "      <td>our God is good</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>6,7</td>\n",
       "      <td>4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106997</th>\n",
       "      <td>is a legend for</td>\n",
       "      <td>the rick and morty intro</td>\n",
       "      <td>on his new album</td>\n",
       "      <td>twaiku</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143137 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0                                 1  \\\n",
       "39745           my uncle HAD to       have a brother who then had   \n",
       "82611      Hating on the Browns            isn't gonna get rid of   \n",
       "73965          i'm sorry but if   this happens i'll stop watching   \n",
       "68681        People gonna treat         you how you allow them to   \n",
       "132423  Patrick we should start   working out Me you should start   \n",
       "...                         ...                               ...   \n",
       "50742         Championship game              tomorrow WE ABOUT TO   \n",
       "40233             it is okay if             you ignoring me but i   \n",
       "87743             imagine being        here from the beginning at   \n",
       "93534        What a healing and         deliverance praise Indeed   \n",
       "106997          is a legend for          the rick and morty intro   \n",
       "\n",
       "                          2  source 0_syllables 1_syllables 2_syllables  \n",
       "39745     kids what a loser  twaiku           5           7           5  \n",
       "82611   your man tits Jason  twaiku           5           7           5  \n",
       "73965    college volleyball  twaiku           5           7           5  \n",
       "68681   treat you everytime  twaiku           5           7           5  \n",
       "132423  minding ur business  twaiku           5           7           5  \n",
       "...                     ...     ...         ...         ...         ...  \n",
       "50742            GET THAT W  twaiku           5           7           5  \n",
       "40233    want you notice me  twaiku           5           7           5  \n",
       "87743     witnessing them i  twaiku           5           7           5  \n",
       "93534       our God is good  twaiku           5         6,7         4,5  \n",
       "106997     on his new album  twaiku           5           7           5  \n",
       "\n",
       "[143137 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(str(haiku_path))\n",
    "df = df.sample(frac=sample_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Input for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:05:11.924966Z",
     "start_time": "2020-03-13T04:05:09.857851Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172759 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \n",
       "0                   white cross           5           5           2  \n",
       "1             i think of lilacs           2           5           5  \n",
       "1             i think of lilacs           3           5           5  \n",
       "2                     breakfast           3           4           2  \n",
       "2                     breakfast           4           4           2  \n",
       "...                         ...         ...         ...         ...  \n",
       "143132    what you said neither           5           7           5  \n",
       "143133   inclined to think both           5           7           5  \n",
       "143134         like Theresa May           5           7           5  \n",
       "143135           into Democrats           5           7           5  \n",
       "143136  blood is loud Talk soon           5           7           5  \n",
       "\n",
       "[172759 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate lines with ambiguous syllable counts\n",
    "# (syllable counts where there is a comma because\n",
    "# multiple pronounciations are acceptable)\n",
    "\n",
    "lines = set([0, 1, 2])\n",
    "\n",
    "for i in range(3):\n",
    "    lines.remove(i)\n",
    "    df = df[[\n",
    "        '0', '1', '2',\n",
    "        #'1_syllables', '2_syllables'\n",
    "    ] + ['%s_syllables' % j for j in lines]].join(\n",
    "        df['%s_syllables' % i].str.split(\n",
    "            ',', expand=True\n",
    "        ).stack(-1).reset_index(\n",
    "            level=1, drop=True\n",
    "        ).rename('%s_syllables' % i)\n",
    "    ).drop_duplicates()\n",
    "    lines.add(i)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:05:41.930595Z",
     "start_time": "2020-03-13T04:05:41.658072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167632 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \n",
       "0                   white cross           5           5           2  \n",
       "1             i think of lilacs           2           5           5  \n",
       "1             i think of lilacs           3           5           5  \n",
       "2                     breakfast           3           4           2  \n",
       "2                     breakfast           4           4           2  \n",
       "...                         ...         ...         ...         ...  \n",
       "143132    what you said neither           5           7           5  \n",
       "143133   inclined to think both           5           7           5  \n",
       "143134         like Theresa May           5           7           5  \n",
       "143135           into Democrats           5           7           5  \n",
       "143136  blood is loud Talk soon           5           7           5  \n",
       "\n",
       "[167632 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop samples that are longer that the 99th percentile of length\n",
    "\n",
    "max_line_length = int(max([df['%s' % i].str.len().quantile(.99) for i in range(3)]))\n",
    "df = df[\n",
    "    (df['0'].str.len() <= max_line_length) & \n",
    "    (df['1'].str.len() <= max_line_length) & \n",
    "    (df['2'].str.len() <= max_line_length)\n",
    "].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:05:43.121619Z",
     "start_time": "2020-03-13T04:05:42.066263Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0_syllables</th>\n",
       "      <th>1_syllables</th>\n",
       "      <th>2_syllables</th>\n",
       "      <th>0_in</th>\n",
       "      <th>0_out</th>\n",
       "      <th>1_in</th>\n",
       "      <th>1_out</th>\n",
       "      <th>2_in</th>\n",
       "      <th>2_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Memorial Day --</td>\n",
       "      <td>a shadow for each</td>\n",
       "      <td>white cross</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring rain -</td>\n",
       "      <td>as the doctor speaks</td>\n",
       "      <td>i think of lilacs</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring moonset --</td>\n",
       "      <td>a rice ball for</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143132</th>\n",
       "      <td>I'm not asking did</td>\n",
       "      <td>you say it nor clarify</td>\n",
       "      <td>what you said neither</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143133</th>\n",
       "      <td>You are truly a</td>\n",
       "      <td>moron or a liar I'm</td>\n",
       "      <td>inclined to think both</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143134</th>\n",
       "      <td>Ain't no selfie on</td>\n",
       "      <td>this earth that's gonna make me</td>\n",
       "      <td>like Theresa May</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143135</th>\n",
       "      <td>is doing a great</td>\n",
       "      <td>job turning Independents</td>\n",
       "      <td>into Democrats</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143136</th>\n",
       "      <td>Wanted to send a</td>\n",
       "      <td>quick follow up on if the</td>\n",
       "      <td>blood is loud Talk soon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167632 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                                 1  \\\n",
       "0          Memorial Day --                 a shadow for each   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "1            spring rain -              as the doctor speaks   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "2        spring moonset --                   a rice ball for   \n",
       "...                    ...                               ...   \n",
       "143132  I'm not asking did            you say it nor clarify   \n",
       "143133     You are truly a               moron or a liar I'm   \n",
       "143134  Ain't no selfie on   this earth that's gonna make me   \n",
       "143135    is doing a great          job turning Independents   \n",
       "143136    Wanted to send a         quick follow up on if the   \n",
       "\n",
       "                              2 0_syllables 1_syllables 2_syllables  \\\n",
       "0                   white cross           5           5           2   \n",
       "1             i think of lilacs           2           5           5   \n",
       "1             i think of lilacs           3           5           5   \n",
       "2                     breakfast           3           4           2   \n",
       "2                     breakfast           4           4           2   \n",
       "...                         ...         ...         ...         ...   \n",
       "143132    what you said neither           5           7           5   \n",
       "143133   inclined to think both           5           7           5   \n",
       "143134         like Theresa May           5           7           5   \n",
       "143135           into Democrats           5           7           5   \n",
       "143136  blood is loud Talk soon           5           7           5   \n",
       "\n",
       "                                                     0_in  \\\n",
       "0       MMemorial Day --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       sspring rain -\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       sspring moonset --\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  II'm not asking did\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  YYou are truly a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  AAin't no selfie on\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iis doing a great\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  WWanted to send a\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    0_out  \\\n",
       "0       Memorial Day --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       spring rain -\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       spring moonset --\\na\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  I'm not asking did\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143133  You are truly a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143134  Ain't no selfie on\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  is doing a great\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  Wanted to send a\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     1_in  \\\n",
       "0       aa shadow for each\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       aas the doctor speaks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       aa rice ball for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132    you say it nor clarify\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133    moron or a liar I'm\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134    this earth that's gonna make me\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135    job turning Independents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136    quick follow up on if the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                    1_out  \\\n",
       "0       a shadow for each\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "1       as the doctor speaks\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       a rice ball for\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132   you say it nor clarify\\nw\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133   moron or a liar I'm\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134   this earth that's gonna make me\\nl\\n\\n\\n\\n\\n\\...   \n",
       "143135   job turning Independents\\ni\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143136   quick follow up on if the\\nb\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                     2_in  \\\n",
       "0       wwhite cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "1       ii think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "2       bbreakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "...                                                   ...   \n",
       "143132  wwhat you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "143133  iinclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143134  llike Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143135  iinto Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "143136  bblood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "                                                    2_out  \n",
       "0       white cross\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "1       i think of lilacs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "2       breakfast\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "...                                                   ...  \n",
       "143132  what you said neither\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "143133  inclined to think both\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143134  like Theresa May\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143135  into Democrats\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "143136  blood is loud Talk soon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  \n",
       "\n",
       "[167632 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the lines to the max line length with new lines\n",
    "for i in range(3):\n",
    "    # For input, duplicate the first character\n",
    "    # TODO - Why?\n",
    "    df['%s_in' % i] = (df[str(i)].str[0] + df[str(i)]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "    # \n",
    "    #df['%s_out' % i] = df[str(i)].str.pad(max_line_len, 'right', '\\n') + ('\\n' if i == 2 else df[str(i+1)].str[0])\n",
    "    \n",
    "    # TODO - trying to add the next line's first character before the line breaks\n",
    "    if i == 2: # If it's the last line\n",
    "        df['%s_out' % i] = df[str(i)].str.pad(max_line_length+2, 'right', '\\n')\n",
    "    else: \n",
    "        # If it's the first or second line, add the first character of the next line to the end of this line.\n",
    "        # This helps with training so that the next RNN has a better chance of getting the first character right.\n",
    "        df['%s_out' % i] = (df[str(i)] + '\\n' + df[str(i+1)].str[0]).str.pad(max_line_length+2, 'right', '\\n')\n",
    "    \n",
    "max_line_length += 2\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:06:17.380823Z",
     "start_time": "2020-03-13T04:05:50.174069Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = df[['0_in', '1_in', '2_in']].values\n",
    "\n",
    "tokenizer = Tokenizer(filters='', char_level=True)\n",
    "tokenizer.fit_on_texts(inputs.flatten())\n",
    "n_tokens = len(tokenizer.word_counts) + 1\n",
    "\n",
    "# X is the input for each line in sequences of one-hot-encoded values\n",
    "X = np_utils.to_categorical([\n",
    "    tokenizer.texts_to_sequences(inputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "outputs = df[['0_out', '1_out', '2_out']].values\n",
    "\n",
    "# Y is the output for each line in sequences of one-hot-encoded values\n",
    "Y = np_utils.to_categorical([\n",
    "    tokenizer.texts_to_sequences(outputs[:,i]) for i in range(3)\n",
    "], num_classes=n_tokens)\n",
    "\n",
    "# X_syllables is the count of syllables for each line\n",
    "X_syllables = df[['0_syllables', '1_syllables', '2_syllables']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:06:17.474322Z",
     "start_time": "2020-03-13T04:06:17.466623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output_all_data_test_2/metadata.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump([latent_dim, n_tokens, max_line_length, tokenizer], str(output_dir / 'metadata.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-13T04:07:22.331218Z",
     "start_time": "2020-03-13T04:06:32.884695Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/liboxian/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 150868 samples, validate on 16764 samples\n",
      "Epoch 1/10\n",
      "  2048/150868 [..............................] - ETA: 39:21 - loss: 7.8077 - output_line_0_loss: 2.4026 - output_line_1_loss: 2.8941 - output_line_2_loss: 2.5110"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-95930e40b45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_syllables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_syllables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m ], [Y[0], Y[1], Y[2]], batch_size=64, epochs=epochs, validation_split=.1, callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepfacelab/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_model, lstm, lines, inputs, outputs = create_training_model(latent_dim, n_tokens)\n",
    "\n",
    "filepath = str(output_dir / (\"%s-{epoch:02d}-{loss:.2f}-{val_loss:.2f}.hdf5\" % latent_dim))\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "csv_logger = CSVLogger(str(output_dir / 'training_log.csv'), append=True, separator=',')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]\n",
    "\n",
    "training_model.fit([\n",
    "    X[0], X_syllables[:,0], \n",
    "    X[1], X_syllables[:,1], \n",
    "    X[2], X_syllables[:,2]\n",
    "], [Y[0], Y[1], Y[2]], batch_size=64, epochs=epochs, validation_split=.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(lstm, lines, tf_session, tokenizer, n_tokens, max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.generate_haiku()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfacelab",
   "language": "python",
   "name": "deepfacelab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
